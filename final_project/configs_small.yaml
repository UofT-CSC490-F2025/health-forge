model:
  input_dim: 1024
  hidden_dim: 2048
  text_embed_dim: 768
  num_layers: 6 # (3 → 6)
  num_heads: 8 # safer for optional attention
  dropout: 0.2
  use_attention: True # leave off unless needed

data_utils:
  test_split: 0.1
  T: 1000
  lambda_min: -20
  lambda_max: 20
  embed_drop_prob: 0.05
  schedule: "linear"
  num_workers: 8
  batch_size: 256

trainer:
  lr: 0.0002 # (instead of 1e-5)
  lr_schedule: "cosine" # NEW
  warmup_steps: 500 # NEW
  ema_decay: 0.999 # NEW
  ema_update_interval: 1 # NEW
  num_epochs: 200
  save_path: best_diffusion_model.pt

sampler:
  guidance_scale: 3.0 # (0.3 → 0.6 for stronger conditioning)
  var_interpolation_coeff: 0.1
