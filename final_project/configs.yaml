model:
  input_dim: 1024
  hidden_dim: 4096
  text_embed_dim: 768
  num_layers: 8
  num_heads: 8 # safer for optional attention
  dropout: 0.2
  use_attention: True # leave off unless needed

data_utils:
  test_split: 0.1
  T: 1000
  lambda_min: -20
  lambda_max: 20
  embed_drop_prob: 0.05
  schedule: "linear"
  num_workers: 8
  batch_size: 1024

trainer:
  lr: 0.0002 # (instead of 1e-5)
  lr_schedule: "cosine" # NEW
  warmup_steps: 500 # NEW
  ema_decay: 0.999 # NEW
  ema_update_interval: 1 # NEW
  num_epochs: 13 #512
  save_path: best_diffusion_model.pt

sampler:
  guidance_scale: 1.0 #3.0
  var_interpolation_coeff: 0.1
