model:
  input_dim: 8
  hidden_dim: 512
  text_embed_dim: 384
  num_layers: 12
  num_heads: 8
  dropout: 0.1
  use_attention: False

data_utils:
  test_split: 0.2
  T: 1000
  lambda_min: -20
  lambda_max: 20
  embed_drop_prob: 0.1
  schedule: 'linear'
  num_workers: 4
  batch_size: 8

trainer:
  lr: 0.0001
  num_epochs: 20000
  save_path: best_diffusion_model.pt

sampler:
  guidance_scale: 0.3
  var_interpolation_coeff: 0.3
  text_embedding_model_name: all-MiniLM-L6-v2
